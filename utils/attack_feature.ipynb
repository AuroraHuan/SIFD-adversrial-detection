{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Sized\n",
    "import warnings\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "from transformers import  BertForMaskedLM\n",
    "from tqdm import tqdm \n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split \n",
    "from transformers import BertForSequenceClassification, AdamW \n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, RobertaForSequenceClassification\n",
    "from transformers import DebertaTokenizer, DebertaModel, DebertaConfig, DebertaForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup \n",
    "\n",
    "import copy\n",
    "import numpy as np \n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import pickle\n",
    "import nltk\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "supported_pos_tags = [#nltk词性标注中的一些类别\n",
    "    #'CC',  # coordinating conjunction, like \"and but neither versus whether yet so\"  #并列连词\n",
    "    'JJ',  # Adjective, like \"second ill-mannered\" #组合式形容词\n",
    "    'JJR',  # Adjective, comparative, like \"colder\" #形容词比较级\n",
    "    'JJS',  # Adjective, superlative, like \"cheapest\" 形容词最高级\n",
    "    'NN',  # Noun, singular or mass #名词 单数\n",
    "    'NNS',  # Noun, plural #名词复数\n",
    "    'NNP',  # Proper noun, singular #专有名词单数\n",
    "    'NNPS',  # Proper noun, plural  #专有名词复数\n",
    "    'RB',  # Adverb#副词\n",
    "    'RBR',  # Adverb, comparative, like \"lower heavier\"\n",
    "    'RBS',  # Adverb, superlative, like \"best biggest\"\n",
    "    'VB',  # Verb, base form\n",
    "    'VBD',  # Verb, past tense 动词过去式\n",
    "    'VBG',  # Verb, gerund or present participle 动名词/现在分词\n",
    "    'VBN',  # Verb, past participle #过去分词\n",
    "    'VBP',  # Verb, non-3rd person singular present #非第三人称单数\n",
    "    'VBZ',  # Verb, 3rd person singular present #第三人称单数\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  get_pos_vocabulary():\n",
    "    file_name = '/data/zhanghData/AttentionDefense/data/words_pos.dict'\n",
    "    with open(file_name, 'rb') as f:\n",
    "            pos_vocabulary = pickle.load(f)\n",
    "    return pos_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28301\n"
     ]
    }
   ],
   "source": [
    "pos_vocabulary = get_pos_vocabulary()\n",
    "print(len(pos_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'ain', 'all', 'almost', \n",
    "    'alone', 'along', 'already', 'also', 'although', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anyhow', \n",
    "    'anyone', 'anything', 'anyway', 'anywhere', 'are', 'aren', \"aren't\", 'around', 'as', 'at', 'back', 'been', 'before',\n",
    "     'beforehand', 'behind', 'being', 'below', 'beside', 'besides', 'between', 'beyond', 'both', 'but', 'by', 'can', 'cannot',\n",
    "      'could', 'couldn', \"couldn't\", 'd', 'didn', \"didn't\", 'doesn', \"doesn't\", 'don', \"don't\", 'down', 'due', 'during', \n",
    "      'either', 'else', 'elsewhere', 'empty', 'enough', 'even', 'ever', 'everyone', 'everything', 'everywhere', 'except', \n",
    "      'first', 'for', 'former', 'formerly', 'from', 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'he', 'hence', \n",
    "      'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', \n",
    "      'hundred', 'i', 'if', 'in', 'indeed', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'latter', \n",
    "      'latterly', 'least', 'll', 'may', 'me', 'meanwhile', 'mightn', \"mightn't\", 'mine', 'more', 'moreover', 'most', 'mostly', \n",
    "      'must', 'mustn', \"mustn't\", 'my', 'myself', 'namely', 'needn', \"needn't\", 'neither', 'never', 'nevertheless', 'next', \n",
    "      'no', 'nobody', 'none', 'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'o', 'of', 'off', 'on', 'once', 'one', 'only', \n",
    "      'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'per', 'please', 's', 'same', \n",
    "      'shan', \"shan't\", 'she', \"she's\", \"should've\", 'shouldn', \"shouldn't\", 'somehow', 'something', 'sometime', 'somewhere', \n",
    "      'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', \n",
    "      'thereafter', 'thereby', 'therefore', 'therein', 'thereupon', 'these', 'they', 'this', 'those', 'through', 'throughout', \n",
    "      'thru', 'thus', 'to', 'too', 'toward', 'towards', 'under', 'unless', 'until', 'up', 'upon', 'used', 've', 'was', 'wasn', \n",
    "      \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'whatever', 'when', 'whence', 'whenever', 'where', 'whereafter', \n",
    "      'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', 'whoever', 'whole', \n",
    "      'whom', 'whose', 'why', 'with', 'within', 'without', 'won', \"won't\", 'would', 'wouldn', \"wouldn't\", 'y', 'yet', 'you',\n",
    "       \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', '', ';', 'because', 'tv', \"'s\", '--', \n",
    "       'wo', 'some', '-', 'de', 'ca', 'so', \"'ll\", \"'m\", 'despite', 'two', 'should', 'might', \"'d\", 'inside', 'three', 'be', 'like',\n",
    "        ')', '.', '...', '``', 'though', 'will', \"'\", 'each', \"''\", ',', 'since', 'every', '?', '(', ':', '`', 'us', 'go', '!', 'do',\n",
    "        'I','So','\"','|']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2, output_attentions=False, output_hidden_states=False)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "model.cuda()\n",
    "load_model_path = '/data/zhanghData/AttentionDefense/save_models/imdb_bert/base/best.pt'\n",
    "model.load_state_dict(torch.load(load_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/zhangh/AttentionDefense/save_models/imdb_roberta/base/best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cfb1fdecc211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mload_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/zhangh/AttentionDefense/save_models/imdb_roberta/base/best.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mload_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/defense/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/defense/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/defense/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/zhangh/AttentionDefense/save_models/imdb_roberta/base/best.pt'"
     ]
    }
   ],
   "source": [
    "model_type = 'roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_type)\n",
    "config = RobertaConfig.from_pretrained(model_type, num_labels=2, output_attentions=False, output_hidden_states=False, \\\n",
    "                attention_probs_dropout_prob=0, hidden_dropout_prob=0.1)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_type, config=config) \n",
    "model.cuda()\n",
    "load_model_path = '/home/zhangh/AttentionDefense/save_models/imdb_roberta/base/best.pt'\n",
    "model.load_state_dict(torch.load(load_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##攻击文件导入\n",
    "from attack_instance import read_adv_files\n",
    "atk_path = '/data/zhanghData/AttentionDefense/save_results/attacks/imdb_bert/base/pwws_0.1_100.csv'\n",
    "atk_instances = read_adv_files(atk_path)\n",
    "all_orig_texts = [atk.orig_text for atk in atk_instances]\n",
    "all_perd_texts = [atk.perd_text for atk in atk_instances]\n",
    "all_gold_labels = [atk.ground for atk in atk_instances]\n",
    "all_changed_indices = [atk.atk_indices for atk in atk_instances]\n",
    "all_change_words = [atk.atk_changes for atk in atk_instances] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21a3d357b98ac7685015af18f21083f1b9afddeabab55ab3c0b23492acc2baa4"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('defense')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
