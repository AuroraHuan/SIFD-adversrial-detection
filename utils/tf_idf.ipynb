{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "# 赋给语料库中每个词(不重复的词)一个整数id\n",
    "# dic = corpora.Dictionary(words_list)\n",
    "# new_corpus = [dic.doc2bow(words) for words in words_list]\n",
    "# # 元组中第一个元素是词语在词典中对应的id，第二个元素是词语在文档中出现的次数\n",
    "# print(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Data...\n",
      "Done, load 67349 datas from sst2 train dataset.\n"
     ]
    }
   ],
   "source": [
    "from data_utils import get_sst_data\n",
    "train_texts,train_labels = get_sst_data(type='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67349\n"
     ]
    }
   ],
   "source": [
    "words_list = [seq[:-1].replace('\\n', '').lower().split(\" \") for seq in train_texts]\n",
    "dic = corpora.Dictionary(words_list)\n",
    "new_corpus = [dic.doc2bow(words) for words in words_list]\n",
    "# 元组中第一个元素是词语在词典中对应的id，第二个元素是词语在文档中出现的次数\n",
    "print(len(new_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型并保存\n",
    "from gensim import models\n",
    "tfidf = models.TfidfModel(new_corpus)\n",
    "tfidf_path = \"/data/zhanghData/AttentionDefense/data/sst2_tfidf.model\"\n",
    "tfidf.save(tfidf_path)\n",
    "# 载入模型\n",
    "tfidf = models.TfidfModel.load(tfidf_path)\n",
    "# 使用这个训练好的模型得到单词的tfidf值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hide new secretions from the parental units \n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)]\n",
      "contains no wit , only labored gags \n",
      "[(7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)]\n",
      "that loves its characters and communicates something rather beautiful about human nature \n",
      "[(14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1)]\n",
      "[[(0, 0.18672363714796544), (1, 0.43681290795631095), (2, 0.2372172822629845), (3, 0.46564405970051354), (4, 0.5161759382471517), (5, 0.06283171495376884), (6, 0.48046530967908124)], [(7, 0.08578942479848845), (8, 0.5208911456479297), (9, 0.4114753686784303), (10, 0.4934927411097709), (11, 0.2775928547914553), (12, 0.29735089608519394), (13, 0.3781404183465558)], [(14, 0.19970740189591735), (15, 0.07414454043424226), (16, 0.3088493012425175), (17, 0.22546417328320306), (18, 0.4868440424810464), (19, 0.2890846434667666), (20, 0.16227087622322192), (21, 0.41167910213222547), (22, 0.35593860621241213), (23, 0.29144184396438194), (24, 0.2552878292805273), (25, 0.12327395371573183)]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vec = []\n",
    "words_tfidf = []\n",
    "for i in range(len(train_texts[:3])):\n",
    "    print(train_texts[i])\n",
    "    words = train_texts[i].lower().split(\" \")\n",
    "    string_bow = dic.doc2bow(words)\n",
    "    print(string_bow)\n",
    "    string_tfidf = tfidf[string_bow]\n",
    "    tfidf_vec.append(string_tfidf)\n",
    "    words_tfidf.append(())\n",
    "# 输出 词语id与词语tfidf值\n",
    "print(tfidf_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic.token2id\n",
    "id2token={}\n",
    "for key,value in dic.token2id.items():\n",
    "    id2token[value]=key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.23396728196594438), (109, 0.5673615629435321), (213, 0.7895316129603303)]\n"
     ]
    }
   ],
   "source": [
    "test_words = \"The funny movie\"\n",
    "string_bow = dic.doc2bow(test_words.lower().split())\n",
    "string_tfidf = tfidf[string_bow]\n",
    "print(string_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n",
      "134\n",
      "5\n",
      "440\n",
      "363\n"
     ]
    }
   ],
   "source": [
    "for w in test_words.lower().split():\n",
    "    print(dic.token2id[w])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "21a3d357b98ac7685015af18f21083f1b9afddeabab55ab3c0b23492acc2baa4"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('defense')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
